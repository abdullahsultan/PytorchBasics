{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\muhammad.sultan\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "from torch import nn\n",
    "from torchvision import transforms\n",
    "\n",
    "from torchinfo import summary\n",
    "\n",
    "from py_scripts import data_setup, engine\n",
    "from HelperFunctions import download_data, set_seeds, plot_loss_curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] data\\pizza_steak_sushi_20_percent directory exists, skipping download.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "WindowsPath('data/pizza_steak_sushi_20_percent')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_20_percent_path = download_data(source=\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi_20_percent.zip\",\n",
    "                                     destination=\"pizza_steak_sushi_20_percent\")\n",
    "\n",
    "data_20_percent_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cpu\" #\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(WindowsPath('data/pizza_steak_sushi_20_percent/train'),\n",
       " WindowsPath('data/pizza_steak_sushi_20_percent/test'))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dir = data_20_percent_path / \"train\"\n",
    "test_dir = data_20_percent_path / \"test\"\n",
    "\n",
    "train_dir, test_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_effnetb2_model(num_classes:int=3, # default output classes = 3 (pizza, steak, sushi)\n",
    "                          seed:int=42):\n",
    "  # 1, 2, 3 Create EffNetB2 pretrained weights, transforms and model\n",
    "  weights = torchvision.models.EfficientNet_B2_Weights.DEFAULT\n",
    "  transforms = weights.transforms()\n",
    "  model = torchvision.models.efficientnet_b2(weights=weights).to(device)\n",
    "\n",
    "  # 4. Freeze all layers in the base model\n",
    "  for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "  # 5. Change classifier head with random seed for reproducibility\n",
    "  torch.manual_seed(seed)\n",
    "  model.classifier = nn.Sequential(\n",
    "      nn.Dropout(p=0.3, inplace=True),\n",
    "      nn.Linear(in_features=1408, out_features=num_classes)\n",
    "  ).to(device)\n",
    "\n",
    "  return model, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "effnetb2, effnetb2_transforms = create_effnetb2_model(num_classes=3,\n",
    "                                                      seed=42)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def create_dataloaders(train_dir: str, test_dir: str, transform: transforms.Compose, batch_size: int):\n",
    "\n",
    "    train_data = datasets.ImageFolder(train_dir, transform=transform)\n",
    "    test_data = datasets.ImageFolder(test_dir, transform=transform)\n",
    "\n",
    "    train_dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "    test_dataloader = DataLoader(test_data, batch_size=batch_size, shuffle=False, pin_memory=True)\n",
    "\n",
    "    return train_dataloader, test_dataloader, train_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader_effnetb2, test_dataloader_effnetb2, train_dataset, test_dataset = create_dataloaders(train_dir=train_dir,\n",
    "                                                                                                 test_dir=test_dir,\n",
    "                                                                                                 transform=effnetb2_transforms,\n",
    "                                                                                                 batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15, 5)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataloader_effnetb2), len(test_dataloader_effnetb2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainAccuracy:: 56.67%\n",
      "TestAccuracy:: 94.09%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 1/5 [00:17<01:08, 17.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainAccuracy:: 79.79%\n",
      "TestAccuracy:: 92.56%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 2/5 [00:33<00:49, 16.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainAccuracy:: 85.00%\n",
      "TestAccuracy:: 97.50%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 3/5 [00:49<00:32, 16.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainAccuracy:: 88.12%\n",
      "TestAccuracy:: 94.09%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 4/5 [01:05<00:16, 16.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainAccuracy:: 91.04%\n",
      "TestAccuracy:: 95.34%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [01:22<00:00, 16.43s/it]\n"
     ]
    }
   ],
   "source": [
    "from py_scripts import engine\n",
    "\n",
    "# Loss function\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.Adam(params=effnetb2.parameters(),\n",
    "                             lr=1e-3)\n",
    "\n",
    "# Training function (engine.py)\n",
    "effnetb2_results = engine.train_model_epochs(model=effnetb2,\n",
    "                                train_dataloader=train_dataloader_effnetb2,\n",
    "                                test_dataloader=test_dataloader_effnetb2,\n",
    "                                epochs=5,\n",
    "                                optimizer=optimizer,\n",
    "                                loss_fn=loss_fn, \n",
    "                                writer = None,\n",
    "                                device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from pathlib import Path\n",
    "\n",
    "def save_model(model: torch.nn.Module,\n",
    "               target_dir: str,\n",
    "               model_name: str):\n",
    "    \n",
    "    # Create target directory\n",
    "    target_dir_path = Path(target_dir)\n",
    "    target_dir_path.mkdir(parents=True,\n",
    "                        exist_ok=True)\n",
    "\n",
    "    # Create model save path\n",
    "    assert model_name.endswith(\".pth\") or model_name.endswith(\".pt\"), \"model_name should end with '.pt' or '.pth'\"\n",
    "    model_save_path = target_dir_path / model_name\n",
    "\n",
    "    # Save the model state_dict()\n",
    "    print(f\"[INFO] Saving model to: {model_save_path}\")\n",
    "    torch.save(obj=model.state_dict(),\n",
    "             f=model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Saving model to: models\\effnetb2.pth\n"
     ]
    }
   ],
   "source": [
    "save_model(model=effnetb2,\n",
    "                 target_dir=\"models\",\n",
    "                 model_name=\"effnetb2.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from timeit import default_timer as timer\n",
    "\n",
    "def predict(img) -> tuple[dict, float]:\n",
    "  # Start a timer\n",
    "  start_time = timer()\n",
    "\n",
    "  # Transform the input image for use with EffNetB2\n",
    "  img = effnetb2_transforms(img).unsqueeze(0) # unsqueeze = add batch dimension on 0th index\n",
    "\n",
    "  # Put model into eval mode, make prediction\n",
    "  effnetb2.eval()\n",
    "  with torch.inference_mode():\n",
    "    # Pass transformed image through the model and turn the prediction logits into probaiblities\n",
    "    pred_probs = torch.softmax(effnetb2(img), dim=1)\n",
    "    \n",
    "  # Create a prediction label and prediction probability dictionary\n",
    "  pred_labels_and_probs = {train_dataset.classes[i]: float(pred_probs[0][i]) for i in range(len(train_dataset.classes))}\n",
    "\n",
    "  # Calculate pred time\n",
    "  end_time = timer()\n",
    "  pred_time = round(end_time - start_time, 4)\n",
    "\n",
    "  # Return pred dict and pred time\n",
    "  return pred_labels_and_probs, pred_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[WindowsPath('data/pizza_steak_sushi_20_percent/test/pizza/1001116.jpg'),\n",
       " WindowsPath('data/pizza_steak_sushi_20_percent/test/pizza/1032754.jpg'),\n",
       " WindowsPath('data/pizza_steak_sushi_20_percent/test/pizza/1067986.jpg'),\n",
       " WindowsPath('data/pizza_steak_sushi_20_percent/test/pizza/129666.jpg'),\n",
       " WindowsPath('data/pizza_steak_sushi_20_percent/test/pizza/1315645.jpg')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Get all test data paths\n",
    "test_data_paths = list(Path(test_dir).glob(\"*/*.jpg\"))\n",
    "test_data_paths[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['data\\\\pizza_steak_sushi_20_percent\\\\test\\\\sushi\\\\472912.jpg'],\n",
       " ['data\\\\pizza_steak_sushi_20_percent\\\\test\\\\pizza\\\\296426.jpg'],\n",
       " ['data\\\\pizza_steak_sushi_20_percent\\\\test\\\\pizza\\\\2111981.jpg']]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "example_list = [[str(filepath)] for filepath in random.sample(test_data_paths, k=3)]\n",
    "example_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "title = \"FV Mini\"\n",
    "\n",
    "# demo = gr.Interface(fn=predict,\n",
    "#                     inputs=gr.Image(type=\"pil\"),\n",
    "#                     outputs=[gr.Label(num_top_classes=3, label=\"Predictions\"), gr.Number(label=\"Prediction time (s)\")],\n",
    "#                     examples=example_list,\n",
    "#                     title=title)\n",
    "# demo.launch() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">MLOPS</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "foodvision_mini_demo_path = Path(\"demos/foodvision_mini/\")\n",
    "\n",
    "if foodvision_mini_demo_path.exists():\n",
    "  shutil.rmtree(foodvision_mini_demo_path)\n",
    "  foodvision_mini_demo_path.mkdir(parents=True,\n",
    "                                  exist_ok=True)\n",
    "else:\n",
    "  foodvision_mini_demo_path.mkdir(parents=True,\n",
    "                                  exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Copying data\\pizza_steak_sushi_20_percent\\test\\sushi\\592799.jpg to demos\\foodvision_mini\\examples\\592799.jpg\n",
      "[INFO] Copying data\\pizza_steak_sushi_20_percent\\test\\steak\\3622237.jpg to demos\\foodvision_mini\\examples\\3622237.jpg\n",
      "[INFO] Copying data\\pizza_steak_sushi_20_percent\\test\\pizza\\2582289.jpg to demos\\foodvision_mini\\examples\\2582289.jpg\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "foodvision_mini_examples_path =  foodvision_mini_demo_path / \"examples\"\n",
    "foodvision_mini_examples_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "foodvision_mini_examples = [Path('data/pizza_steak_sushi_20_percent/test/sushi/592799.jpg'),\n",
    "                            Path('data/pizza_steak_sushi_20_percent/test/steak/3622237.jpg'),\n",
    "                            Path('data/pizza_steak_sushi_20_percent/test/pizza/2582289.jpg')]\n",
    "for example in foodvision_mini_examples:\n",
    "  destination = foodvision_mini_examples_path / example.name\n",
    "  print(f\"[INFO] Copying {example} to {destination}\")\n",
    "  shutil.copy2(src=example,\n",
    "               dst=destination)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['examples/2582289.jpg'], ['examples/3622237.jpg'], ['examples/592799.jpg']]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "example_list = [[\"examples/\" + example] for example in os.listdir(foodvision_mini_examples_path)]\n",
    "example_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Attempting to move models/effnetb2.pth to demos\\foodvision_mini\\effnetb2.pth\n",
      "[INFO] Model move complete.\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "\n",
    "# Create a source path for our target model \n",
    "effnetb2_foodvision_mini_model_path = \"models/effnetb2.pth\"\n",
    "\n",
    "# Create a destination path for our target model\n",
    "effnetb2_foodvision_mini_model_destination = foodvision_mini_demo_path / effnetb2_foodvision_mini_model_path.split(\"/\")[1]\n",
    "\n",
    "# Try to move the model file\n",
    "try:\n",
    "  print(f\"[INFO] Attempting to move {effnetb2_foodvision_mini_model_path} to {effnetb2_foodvision_mini_model_destination}\")\n",
    "\n",
    "  # Move the movel\n",
    "  shutil.move(src=effnetb2_foodvision_mini_model_path,\n",
    "              dst=effnetb2_foodvision_mini_model_destination)\n",
    "  \n",
    "  print(f\"[INFO] Model move complete.\")\n",
    "# If the model has already been moved, check if it exists\n",
    "except:\n",
    "  print(f\"[INFO] No model found at {effnetb2_foodvision_mini_model_path}, perhaps its already been moved?\")\n",
    "  print(f\"[INFO] Model exists at {effnetb2_foodvision_mini_model_destination}: {effnetb2_foodvision_mini_model_destination.exists()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing demos/foodvision_mini/model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile demos/foodvision_mini/model.py\n",
    "def create_effnetb2_model(num_classes:int=3, # default output classes = 3 (pizza, steak, sushi)\n",
    "                          seed:int=42):\n",
    "  # 1, 2, 3 Create EffNetB2 pretrained weights, transforms and model\n",
    "  weights = torchvision.models.EfficientNet_B2_Weights.DEFAULT\n",
    "  transforms = weights.transforms()\n",
    "  model = torchvision.models.efficientnet_b2(weights=weights).to(device)\n",
    "\n",
    "  # 4. Freeze all layers in the base model\n",
    "  for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "  # 5. Change classifier head with random seed for reproducibility\n",
    "  torch.manual_seed(seed)\n",
    "  model.classifier = nn.Sequential(\n",
    "      nn.Dropout(p=0.3, inplace=True),\n",
    "      nn.Linear(in_features=1408, out_features=num_classes)\n",
    "  ).to(device)\n",
    "\n",
    "  return model, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing demos/foodvision_mini/app.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile demos/foodvision_mini/app.py\n",
    "\n",
    "import gradio as gr\n",
    "import os \n",
    "import torch\n",
    "\n",
    "from model import create_effnetb2_model\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "class_names = ['pizza', 'steak', 'sushi']\n",
    "\n",
    "effnetb2, effnetb2_transforms = create_effnetb2_model(\n",
    "    num_classes=3)\n",
    "\n",
    "effnetb2.load_state_dict(\n",
    "    torch.load(\n",
    "        f=\"effnetb2.pth\",\n",
    "        map_location=torch.device(\"cpu\") # load the model to the CPU\n",
    "    )\n",
    ")\n",
    "\n",
    "def predict(img) -> tuple[dict, float]:\n",
    "  # Start a timer\n",
    "  start_time = timer()\n",
    "\n",
    "  # Transform the input image for use with EffNetB2\n",
    "  img = effnetb2_transforms(img).unsqueeze(0) # unsqueeze = add batch dimension on 0th index\n",
    "\n",
    "  # Put model into eval mode, make prediction\n",
    "  effnetb2.eval()\n",
    "  with torch.inference_mode():\n",
    "    # Pass transformed image through the model and turn the prediction logits into probaiblities\n",
    "    pred_probs = torch.softmax(effnetb2(img), dim=1)\n",
    "\n",
    "  # Create a prediction label and prediction probability dictionary\n",
    "  pred_labels_and_probs = {class_names[i]: float(pred_probs[0][i]) for i in range(len(class_names))}\n",
    "\n",
    "  # Calculate pred time\n",
    "  end_time = timer()\n",
    "  pred_time = round(end_time - start_time, 4)\n",
    "\n",
    "  # Return pred dict and pred time\n",
    "  return pred_labels_and_probs, pred_time\n",
    "\n",
    "title = \"FoodVision Mini\"\n",
    "\n",
    "example_list = [[\"examples/\" + example] for example in os.listdir(\"examples\")]\n",
    "\n",
    "demo = gr.Interface(fn=predict, # maps inputs to outputs\n",
    "                    inputs=gr.Image(type=\"pil\"),\n",
    "                    outputs=[gr.Label(num_top_classes=3, label=\"Predictions\"),\n",
    "                             gr.Number(label=\"Prediction time (s)\")],\n",
    "                    examples=example_list,\n",
    "                    title=title)\n",
    "\n",
    "# Launch the demo!\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing demos/foodvision_mini/requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile demos/foodvision_mini/requirements.txt\n",
    "torch==2.2.2\n",
    "torchvision=0.17.2\n",
    "gradio=4.36.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
